import { useCallback, useEffect, useRef, useState } from 'react'
import { Editor } from '@tiptap/react'
import { getSpeechRecognitionService, SpeechRecognitionService } from '@/services/SpeechRecognitionService'
import { VOICE_COMMANDS_CONFIG } from '@/lib/voiceCommandsConfig'
import { 
  processMedicalText, 
  shouldApplyWhisperRefinement,
  extractVoiceCommands,
  reinsertVoiceCommands 
} from '@/utils/medicalTextProcessor'
import { supabase } from '@/integrations/supabase/client'
import { toast } from 'sonner'
import { useWhisperCredits } from './useWhisperCredits'

interface UseDictationReturn {
  isActive: boolean
  status: 'idle' | 'waiting' | 'listening'
  startDictation: () => Promise<MediaStream | null>
  stopDictation: () => void
  
  // Whisper features integradas
  isWhisperEnabled: boolean
  toggleWhisper: () => void
  isTranscribing: boolean
  whisperStats: {
    total: number
    success: number
    failed: number
  }
}

interface TextSegment {
  id: string           // UUID √∫nico
  startPos: number     // Posi√ß√£o inicial no editor
  endPos: number       // Posi√ß√£o final no editor
  webSpeechText: string // Texto do Web Speech
  whisperText?: string // Texto do Whisper (quando retornar)
  status: 'pending' | 'processing' | 'refined'
}

// üÜï FASE 3: Interface para isolamento de √°udio por segmento
interface AudioSegment {
  id: string
  audioChunks: Blob[]
  startTimestamp: number
  endTimestamp: number
  webSpeechText: string
  startPos: number
  endPos: number
}

/**
 * Hook unificado para ditado por voz cont√≠nuo com refinamento Whisper
 * Camada 1: Web Speech API ‚Üí preview em tempo real no TipTap
 * Camada 2: MediaRecorder ‚Üí chunking temporal 3s ‚Üí Whisper ‚Üí substitui√ß√£o progressiva
 */
export function useDictation(editor: Editor | null): UseDictationReturn {
  const [isActive, setIsActive] = useState(false)
  const [status, setStatus] = useState<'idle' | 'waiting' | 'listening'>('idle')
  
  // Whisper credits hook
  const { balance, hasEnoughCredits, checkQuota, refreshBalance } = useWhisperCredits()

  // Refs para sistema de √¢ncora din√¢mica (Web Speech)
  const editorRef = useRef<Editor | null>(null)
  const speechServiceRef = useRef<SpeechRecognitionService | null>(null)
  const anchorRef = useRef<number | null>(null)      // Posi√ß√£o inicial do ditado
  const selectionEndRef = useRef<number | null>(null) // Posi√ß√£o final da sele√ß√£o (se houver)
  const interimLengthRef = useRef<number>(0)          // Tamanho do texto provis√≥rio
  const whisperFallbackToastShownRef = useRef<boolean>(false) // Flag para toast √∫nico

  // üéôÔ∏è Refs para sistema Whisper integrado
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const chunkIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const lastSegmentEndRef = useRef<number>(0)
  const textSegmentsRef = useRef<TextSegment[]>([])
  const isProcessingRef = useRef<boolean>(false) // üîí Mutex para evitar race conditions
  const processingQueueRef = useRef<Array<() => Promise<void>>>([]) // üìã Fila de processamento
  const abortControllerRef = useRef<AbortController | null>(null) // üõë Cancelamento de requests
  
  // üÜï FASE 3: Mapa de segmentos de √°udio isolados
  const audioSegmentsRef = useRef<Map<string, AudioSegment>>(new Map())
  const currentSegmentIdRef = useRef<string | null>(null)
  
  // üÜï FASE 2: Flag para detectar edi√ß√£o manual pelo usu√°rio
  const userEditedRef = useRef<boolean>(false)
  
  // Estados Whisper
  const [isWhisperEnabled, setIsWhisperEnabled] = useState(true)
  const [isTranscribing, setIsTranscribing] = useState(false)
  const [whisperStats, setWhisperStats] = useState({
    total: 0,
    success: 0,
    failed: 0,
  })

  // Sincronizar ref do editor sempre que mudar
  useEffect(() => {
    editorRef.current = editor
  }, [editor])

  /**
   * Escapa caracteres especiais para uso em regex
   */
  const escapeRegex = (str: string): string => {
    return str.replace(/[.*+?^${}()|[\]\\]/g, '\\$&')
  }

  /**
   * Converte Blob de √°udio para base64
   */
  const blobToBase64 = useCallback((blob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader()
      reader.onloadend = () => {
        const base64 = reader.result as string
        const base64Data = base64.split(',')[1]
        resolve(base64Data)
      }
      reader.onerror = reject
      reader.readAsDataURL(blob)
    })
  }, [])

  /**
   * üÜï FASE 1-5: Envia chunk de √°udio para Whisper com reconciliador inteligente
   */
  const sendChunkToWhisper = useCallback(async (params: {
    audioBlob: Blob
    startPos: number
    endPos: number
    webSpeechText: string
  }) => {
    const currentEditor = editorRef.current
    if (!currentEditor) return

    const { audioBlob, startPos, endPos, webSpeechText } = params
    
    const MIN_AUDIO_SIZE = 10 * 1024 // 10KB
    if (audioBlob.size < MIN_AUDIO_SIZE) {
      console.log('‚ö†Ô∏è Audio too short, skipping Whisper')
      // üÜï FASE 4: Fallback - manter texto Web Speech
      return
    }
    
    const MAX_AUDIO_SIZE = 25 * 1024 * 1024 // 25MB
    if (audioBlob.size > MAX_AUDIO_SIZE) {
      console.warn('‚ö†Ô∏è Audio too large:', Math.round(audioBlob.size / 1024 / 1024), 'MB')
      return
    }

    setIsTranscribing(true)
    setWhisperStats(prev => ({ ...prev, total: prev.total + 1 }))

    const segmentId = `segment-${Date.now()}`
    
    textSegmentsRef.current.push({
      id: segmentId,
      startPos,
      endPos,
      webSpeechText,
      status: 'processing'
    })

    const MAX_RETRIES = 3
    let attempt = 0
    let whisperSucceeded = false
    
    while (attempt < MAX_RETRIES) {
      try {
        abortControllerRef.current = new AbortController()
        
        // üÜï FASE 5: Extrair comandos de voz antes de enviar para Whisper
        const { cleanText, commands } = extractVoiceCommands(webSpeechText)
        
        const base64Audio = await blobToBase64(audioBlob)
        console.log(`üé§ Sending to Whisper (attempt ${attempt + 1}/${MAX_RETRIES},`, Math.round(audioBlob.size / 1024), 'KB)')

        const { data, error } = await supabase.functions.invoke('transcribe-audio', {
          body: { 
            audio: base64Audio,
            language: 'pt'
          }
        })

        if (error) throw error

        // Refresh balance after consumption
        if (data?.credits_remaining !== undefined) {
          await refreshBalance()
          
          // Show low balance warning
          if (data.credits_remaining < 10) {
            toast.warning(`‚ö° Saldo baixo: ${data.credits_remaining} cr√©ditos restantes`)
          }
        }

        if (data?.text) {
          let whisperText = processMedicalText(data.text)
          
          // üÜï FASE 5: Reinserir comandos de voz ap√≥s Whisper
          whisperText = reinsertVoiceCommands(whisperText, commands)
          
          console.log('‚úÖ Whisper refined:', whisperText.substring(0, 50) + '...')

          const segment = textSegmentsRef.current.find(s => s.id === segmentId)
          if (segment) {
            segment.whisperText = whisperText
            segment.status = 'refined'

            // üÜï FASE 1: RECONCILIADOR INTELIGENTE
            // Pegar texto atual do editor nas posi√ß√µes originais
            const currentEditorText = currentEditor.state.doc.textBetween(startPos, endPos, ' ', ' ')
            
            // Verificar se usu√°rio editou manualmente
            if (shouldApplyWhisperRefinement(webSpeechText, currentEditorText, whisperText)) {
              // üÜï FASE 2: Usar transaction do TipTap para opera√ß√£o at√¥mica
              currentEditor.view.dispatch(
                currentEditor.state.tr
                  .delete(startPos, endPos)
                  .insertText(whisperText + ' ', startPos)
              )
              
              console.log('üîÑ Whisper APLICADO:', webSpeechText.substring(0, 30), '‚Üí', whisperText.substring(0, 30))

              // Ajustar offsets dos segmentos seguintes
              const lengthDiff = (whisperText.length + 1) - (endPos - startPos)
              if (lengthDiff !== 0) {
                textSegmentsRef.current.forEach(s => {
                  if (s.startPos > endPos) {
                    s.startPos += lengthDiff
                    s.endPos += lengthDiff
                  }
                })
              }
            } else {
              console.log('üö´ Whisper BLOQUEADO - preservando edi√ß√£o manual do usu√°rio')
            }
          }

          setWhisperStats(prev => ({ ...prev, success: prev.success + 1 }))
          whisperSucceeded = true
          break
        }

      } catch (error) {
        attempt++
        console.error(`‚ùå Whisper error (attempt ${attempt}/${MAX_RETRIES}):`, error)
        
        if (attempt < MAX_RETRIES) {
          const backoffMs = Math.pow(2, attempt) * 1000
          console.log(`‚è≥ Retrying in ${backoffMs}ms...`)
          await new Promise(resolve => setTimeout(resolve, backoffMs))
        }
      }
    }
    
    // üÜï FASE 4: FALLBACK AUTOM√ÅTICO
    if (!whisperSucceeded) {
      console.log('‚ö†Ô∏è Whisper failed after all retries - keeping Web Speech text')
      
      // Toast √∫nico por sess√£o de ditado
      if (!whisperFallbackToastShownRef.current) {
        whisperFallbackToastShownRef.current = true
        toast.info('Usando transcri√ß√£o b√°sica. Ative o Whisper AI para termos m√©dicos mais precisos.', {
          duration: 6000,
        })
      }
      
      setWhisperStats(prev => ({ ...prev, failed: prev.failed + 1 }))
      // Texto Web Speech j√° est√° no editor, n√£o fazer nada
    }
    
    setIsTranscribing(false)
    abortControllerRef.current = null
  }, [blobToBase64])

  /**
   * üÜï FASE 2: Enfileira processamento Whisper para evitar race conditions
   */
  const enqueueWhisperProcessing = useCallback((params: {
    audioBlob: Blob
    startPos: number
    endPos: number
    webSpeechText: string
  }) => {
    const task = async () => {
      await sendChunkToWhisper(params)
    }
    
    processingQueueRef.current.push(task)
    processNextInQueue()
  }, [sendChunkToWhisper])

  /**
   * üÜï FASE 2: Processa pr√≥ximo item da fila com mutex
   */
  const processNextInQueue = useCallback(async () => {
    // üîí Mutex: se j√° est√° processando, n√£o inicia outro
    if (isProcessingRef.current || processingQueueRef.current.length === 0) {
      return
    }
    
    isProcessingRef.current = true
    const task = processingQueueRef.current.shift()
    
    if (task) {
      try {
        await task()
      } catch (error) {
        console.error('‚ùå Error processing queued task:', error)
      } finally {
        isProcessingRef.current = false
        // Processar pr√≥ximo item da fila
        processNextInQueue()
      }
    } else {
      isProcessingRef.current = false
    }
  }, [])

  /**
   * Apaga a √∫ltima palavra digitada (comando de voz "apagar isso")
   */
  const deleteLastWord = (editor: Editor) => {
    const { state } = editor
    const { from, to } = state.selection
    const textBefore = state.doc.textBetween(0, from, ' ', ' ')
    
    // Encontrar in√≠cio da √∫ltima palavra
    const match = textBefore.match(/\s*(\S+)\s*$/)
    if (match) {
      const wordLength = match[0].length
      const deleteFrom = from - wordLength
      editor.commands.deleteRange({ from: deleteFrom, to })
    }
  }

  /**
   * Verifica se a pr√≥xima palavra deve come√ßar com mai√∫scula
   * baseado no contexto do editor
   */
  const shouldCapitalizeNext = (editor: Editor, position: number): boolean => {
    // Se posi√ß√£o √© 0 ou 1, √© in√≠cio do documento
    if (position <= 1) return true
    
    // Pegar texto antes da posi√ß√£o atual
    const textBefore = editor.state.doc.textBetween(0, position, ' ', ' ')
    if (!textBefore.trim()) return true
    
    // Verificar se termina com pontua√ß√£o de fim de frase
    const trimmed = textBefore.trim()
    const lastChar = trimmed.charAt(trimmed.length - 1)
    
    return ['.', '!', '?', '\n'].includes(lastChar)
  }

  /**
   * Aplica capitaliza√ß√£o inteligente ao texto
   * - Primeira letra mai√∫scula no in√≠cio
   * - Primeira letra mai√∫scula ap√≥s . ! ?
   */
  const applyCapitalization = (text: string, isStartOfDocument: boolean): string => {
    if (!text.trim()) return text
    
    let result = text
    
    // Capitalizar primeira letra se in√≠cio de documento ou par√°grafo
    if (isStartOfDocument) {
      result = result.charAt(0).toUpperCase() + result.slice(1)
    }
    
    // Capitalizar ap√≥s pontua√ß√£o de fim de frase (. ! ?)
    result = result.replace(/([.!?]\s+)([a-z√°√†√¢√£√©√®√™√≠√¨√Æ√≥√≤√¥√µ√∫√π√ª√ß])/gi, (match, punct, letter) => {
      return punct + letter.toUpperCase()
    })
    
    // Capitalizar ap√≥s quebra de linha
    result = result.replace(/(\n\s*)([a-z√°√†√¢√£√©√®√™√≠√¨√Æ√≥√≤√¥√µ√∫√π√ª√ß])/gi, (match, newline, letter) => {
      return newline + letter.toUpperCase()
    })
    
    return result
  }

  /**
   * Divide texto por comandos estruturais (linha/par√°grafo)
   * Retorna array de segmentos para processamento separado
   */
  interface VoiceSegment {
    type: 'text' | 'hard_break' | 'split_block';
    content?: string;
  }

  const splitByStructuralCommands = (text: string): VoiceSegment[] => {
    const segments: VoiceSegment[] = []
    
    // Comandos estruturais ordenados por tamanho (maior primeiro)
    const structuralCommands = VOICE_COMMANDS_CONFIG
      .filter(cmd => cmd.action === 'hard_break' || cmd.action === 'split_block')
      .sort((a, b) => b.command.length - a.command.length)
    
    let remaining = text
    
    while (remaining.length > 0) {
      let foundCommand = false
      
      for (const cmd of structuralCommands) {
        const regex = new RegExp(`(^|\\s)(${escapeRegex(cmd.command)})(\\s|$)`, 'i')
        const match = remaining.match(regex)
        
        if (match && match.index !== undefined) {
          // Texto antes do comando
          const beforeText = remaining.substring(0, match.index + match[1].length)
          if (beforeText.trim()) {
            segments.push({ type: 'text', content: beforeText.trim() })
          }
          
          // Comando estrutural
          segments.push({ 
            type: cmd.action as 'hard_break' | 'split_block'
          })
          
          // Continuar processando o restante
          remaining = remaining.substring(match.index + match[0].length)
          foundCommand = true
          break
        }
      }
      
      if (!foundCommand) {
        // N√£o encontrou mais comandos, adicionar texto restante
        if (remaining.trim()) {
          segments.push({ type: 'text', content: remaining.trim() })
        }
        break
      }
    }
    
    return segments
  }

  /**
   * Substitui comandos de pontua√ß√£o (n√£o estruturais) no texto
   */
  const replacePunctuationCommands = (text: string): string => {
    let replaced = text
    
    // Comandos de pontua√ß√£o ordenados por tamanho
    const punctuationCommands = VOICE_COMMANDS_CONFIG
      .filter(cmd => cmd.action === 'insert_text' && !cmd.followedBy)
      .sort((a, b) => b.command.length - a.command.length)
    
    for (const cmd of punctuationCommands) {
      const regex = new RegExp(escapeRegex(cmd.command), 'gi')
      
      if (regex.test(replaced)) {
        regex.lastIndex = 0
        
        if (cmd.parameters?.text) {
          replaced = replaced.replace(regex, cmd.parameters.text)
        }
      }
    }

    // Normalizar espa√ßos
    replaced = replaced.replace(/\s+([.,;:!?])/g, '$1')
    replaced = replaced.replace(/([.,;:!?])(?=[^\s])/g, '$1 ')
    replaced = replaced.replace(/  +/g, ' ')

    return replaced
  }

  /**
   * Processa entrada de voz usando comandos nativos do TipTap
   */
  const processVoiceInput = (transcript: string, currentEditor: Editor) => {
    const segments = splitByStructuralCommands(transcript)
    
    for (const segment of segments) {
      if (segment.type === 'text' && segment.content) {
        // Processar pontua√ß√£o
        let processedText = replacePunctuationCommands(segment.content)
        
        // üÜï APLICAR CORRE√á√ïES M√âDICAS EM TEMPO REAL
        processedText = processMedicalText(processedText)
        
        // Verificar capitaliza√ß√£o baseado na posi√ß√£o atual do cursor
        const shouldCapitalize = shouldCapitalizeNext(currentEditor, currentEditor.state.selection.from)
        processedText = applyCapitalization(processedText, shouldCapitalize)
        
        // Inserir texto
        currentEditor.chain().focus().insertContent(processedText + ' ').run()
        
      } else if (segment.type === 'hard_break') {
        // Comando nativo TipTap para quebra de linha
        currentEditor.chain().focus().setHardBreak().run()
        
      } else if (segment.type === 'split_block') {
        // Comando nativo TipTap para novo par√°grafo
        currentEditor.chain().focus().splitBlock().run()
      }
    }
    
    // Processar comandos compostos (ex: "ponto par√°grafo")
    const compositeCmd = VOICE_COMMANDS_CONFIG.find(
      cmd => cmd.followedBy && transcript.toLowerCase().includes(cmd.command.toLowerCase())
    )
    
    if (compositeCmd?.followedBy === 'split_block') {
      currentEditor.chain().focus().splitBlock().run()
    }
  }

  /**
   * üÜï FASE 6: Handlers est√°veis usando useRef para evitar re-registros
   */
  const handleInterimTranscriptRef = useRef<(transcript: string) => void>(() => {})
  const handleFinalTranscriptRef = useRef<(transcript: string) => void>(() => {})

  /**
   * Handler para transcri√ß√µes provis√≥rias (em tempo real)
   * Mostra preview com marcadores visuais para comandos estruturais
   */
  handleInterimTranscriptRef.current = useCallback((transcript: string) => {
    const currentEditor = editorRef.current
    if (!currentEditor || !transcript.trim()) return

    // Se n√£o tem √¢ncora ainda, verificar se h√° sele√ß√£o
    if (anchorRef.current === null) {
      const { from, to } = currentEditor.state.selection
      anchorRef.current = from
      
      if (from !== to) {
        selectionEndRef.current = to
        currentEditor.commands.deleteRange({ from, to })
        console.log('üóëÔ∏è Selection deleted:', { from, to })
      }
    }

    const anchor = anchorRef.current
    const currentInterimLength = interimLengthRef.current

    // Processar texto para preview (com marcadores visuais)
    let previewText = transcript
    
    // üÜï APLICAR CORRE√á√ïES M√âDICAS NO PREVIEW TAMB√âM (tempo real)
    previewText = processMedicalText(previewText)
    
    // Substituir comandos estruturais por marcadores visuais
    previewText = previewText.replace(/nova linha|pr√≥xima linha|linha/gi, ' [‚Üµ] ')
    previewText = previewText.replace(/novo par√°grafo|pr√≥ximo par√°grafo|par√°grafo/gi, ' [¬∂] ')
    
    // Processar pontua√ß√£o
    previewText = replacePunctuationCommands(previewText)
    
    // Verificar se deve capitalizar
    const shouldCapitalize = shouldCapitalizeNext(currentEditor, anchor)
    const capitalizedText = applyCapitalization(previewText, shouldCapitalize)

    // Substituir texto provis√≥rio anterior pelo novo
    if (currentInterimLength > 0) {
      currentEditor.commands.deleteRange({ 
        from: anchor, 
        to: anchor + currentInterimLength 
      })
    }
    
    currentEditor.commands.insertContentAt(anchor, capitalizedText, {
      updateSelection: true,
    })

    // Atualizar comprimento do texto provis√≥rio
    interimLengthRef.current = capitalizedText.length

    console.log('üìù Interim with markers:', capitalizedText)
  }, [])

  /**
   * Handler para transcri√ß√µes finais (confirmadas)
   * üÜï FASE 2: Corre√ß√£o de posicionamento - capturar ANTES de processVoiceInput
   * üÜï FASE 3: Isolamento de √°udio por segmento
   */
  handleFinalTranscriptRef.current = useCallback((transcript: string) => {
    const currentEditor = editorRef.current
    console.log('‚úÖ Final transcript:', transcript)
    
    if (!currentEditor || !transcript.trim()) return

    // Se n√£o tem √¢ncora ainda, verificar se h√° sele√ß√£o
    if (anchorRef.current === null) {
      const { from, to } = currentEditor.state.selection
      anchorRef.current = from
      
      if (from !== to) {
        selectionEndRef.current = to
        currentEditor.commands.deleteRange({ from, to })
        console.log('üóëÔ∏è Selection deleted on final:', { from, to })
      }
    }

    const anchor = anchorRef.current ?? currentEditor.state.selection.from
    const currentInterimLength = interimLengthRef.current

    const lowerTranscript = transcript.toLowerCase().trim()
    
    // Verificar comandos especiais primeiro
    for (const cmd of VOICE_COMMANDS_CONFIG) {
      if (lowerTranscript === cmd.command && 
          !['insert_text', 'hard_break', 'split_block'].includes(cmd.action)) {
        if (currentInterimLength > 0) {
          currentEditor.commands.deleteRange({ 
            from: anchor, 
            to: anchor + currentInterimLength 
          })
        }
        anchorRef.current = null
        interimLengthRef.current = 0
        
        switch (cmd.action) {
          case 'delete_word':
            deleteLastWord(currentEditor)
            return
          case 'undo':
            currentEditor.commands.undo()
            return
          case 'redo':
            currentEditor.commands.redo()
            return
          case 'toggle_bold':
            currentEditor.commands.toggleBold()
            return
          case 'toggle_italic':
            currentEditor.commands.toggleItalic()
            return
          case 'toggle_underline':
            currentEditor.commands.toggleUnderline()
            return
        }
      }
    }

    // Remover texto provis√≥rio
    if (currentInterimLength > 0) {
      currentEditor.commands.deleteRange({ 
        from: anchor, 
        to: anchor + currentInterimLength 
      })
    }

    // üÜï FASE 2: CORRE√á√ÉO - Capturar posi√ß√£o ANTES de processVoiceInput
    const webSpeechStartPos = anchor
    
    currentEditor.commands.setTextSelection(anchor)
    
    // Processar usando comandos nativos do TipTap
    processVoiceInput(transcript, currentEditor)

    // üÜï FASE 2: Capturar posi√ß√£o DEPOIS (corrigido)
    const webSpeechEndPos = currentEditor.state.selection.from
    
    // üÜï FASE 3: Associar √°udio √†s posi√ß√µes EXATAS
    if (isWhisperEnabled && audioChunksRef.current.length > 0) {
      const audioBlob = new Blob(audioChunksRef.current)
      
      enqueueWhisperProcessing({
        audioBlob,
        startPos: webSpeechStartPos,
        endPos: webSpeechEndPos,
        webSpeechText: transcript
      })
      
      // üÜï FASE 3: Limpar buffer imediatamente para isolamento
      audioChunksRef.current = []
    }

    // Resetar estado
    anchorRef.current = null
    selectionEndRef.current = null
    interimLengthRef.current = 0

    console.log('‚úèÔ∏è Final processed:', webSpeechStartPos, '->', webSpeechEndPos)
  }, [isWhisperEnabled, enqueueWhisperProcessing])

  /**
   * Inicia grava√ß√£o de √°udio para Whisper
   * üÜï FASE 3: Timeslice aumentado de 100ms para 500ms
   */
  const startAudioRecording = useCallback(async (stream: MediaStream) => {
    if (!isWhisperEnabled) return

    try {
      const SUPPORTED_MIMETYPES = [
        'audio/webm;codecs=opus',
        'audio/webm',
        'audio/ogg;codecs=opus',
        'audio/mp4'
      ]

      const mimeType = SUPPORTED_MIMETYPES.find(type => 
        MediaRecorder.isTypeSupported(type)
      )

      if (!mimeType) {
        throw new Error('Nenhum formato de √°udio suportado')
      }

      const mediaRecorder = new MediaRecorder(stream, { mimeType })
      audioChunksRef.current = []

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      // üÜï FASE 3: Timeslice otimizado de 100ms ‚Üí 500ms (menos overhead)
      mediaRecorder.start(500)
      mediaRecorderRef.current = mediaRecorder

      console.log('üéôÔ∏è Audio recording started for Whisper with', mimeType)
    } catch (error) {
      console.error('‚ùå Failed to start audio recording:', error)
      toast.error('Erro ao iniciar grava√ß√£o de √°udio')
    }
  }, [isWhisperEnabled])

  /**
   * Para grava√ß√£o de √°udio Whisper
   * üÜï FASE 4: Cancela requests em andamento e limpa fila
   */
  const stopAudioRecording = useCallback(() => {
    if (chunkIntervalRef.current) {
      clearInterval(chunkIntervalRef.current)
      chunkIntervalRef.current = null
    }

    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop()
      mediaRecorderRef.current = null
      console.log('üéôÔ∏è Audio recording stopped')
    }
    
    // üÜï FASE 4: Cancelar request Whisper em andamento
    if (abortControllerRef.current) {
      abortControllerRef.current.abort()
      abortControllerRef.current = null
    }
    
    // Limpar fila de processamento
    processingQueueRef.current = []
    isProcessingRef.current = false
    
    audioChunksRef.current = []
    textSegmentsRef.current = []
    lastSegmentEndRef.current = 0
  }, [])

  /**
   * ‚ùå REMOVIDO: Chunking temporal substitu√≠do por sincroniza√ß√£o em handleFinalTranscript
   * A l√≥gica agora captura posi√ß√µes exatas quando Web Speech confirma o texto
   */

  /**
   * Inicia o ditado por voz com captura de audio stream e Whisper
   * üÜï Sem chunking temporal - sincroniza√ß√£o acontece em handleFinalTranscript
   */
  const startDictation = useCallback(async (): Promise<MediaStream | null> => {
    const currentEditor = editorRef.current
    if (!currentEditor || !speechServiceRef.current) {
      console.error('‚ùå Cannot start dictation: editor or speechService not ready')
      return null
    }

    console.log('üé§ Starting unified dictation (Web Speech + Whisper)...')
    
    const result = await speechServiceRef.current.startListeningWithAudio()
    if (result.started) {
      setIsActive(true)
      
      const stream = result.stream
      if (stream && isWhisperEnabled) {
        // Iniciar grava√ß√£o de √°udio para Whisper
        await startAudioRecording(stream)
        // ‚ùå Chunking temporal removido - agora sincronizado com handleFinalTranscript
      }
      
      console.log('‚úì Dictation started successfully with Whisper integration')
      return stream || null
    }
    
    console.error('‚úó Failed to start dictation')
    return null
  }, [isWhisperEnabled, startAudioRecording])

  /**
   * Para o ditado por voz
   * üÜï Sem envio de chunk final - processamento j√° acontece em handleFinalTranscript
   */
  const stopDictation = useCallback(() => {
    if (!speechServiceRef.current) return

    speechServiceRef.current.stopListening()
    setIsActive(false)
    setStatus('idle')
    
    // ‚ùå N√£o envia chunk final - j√° processado em handleFinalTranscript
    
    // Para grava√ß√£o de √°udio e limpa fila
    stopAudioRecording()
    
    // Resetar estado de √¢ncora
    anchorRef.current = null
    selectionEndRef.current = null
    interimLengthRef.current = 0
    whisperFallbackToastShownRef.current = false // Reset toast flag
    
    console.log('üõë Unified dictation stopped')
  }, [stopAudioRecording])

  /**
   * üÜï FASE 6: Callbacks estabilizados - sem depend√™ncias no array
   */
  useEffect(() => {
    const speechService = getSpeechRecognitionService()
    speechServiceRef.current = speechService

    const statusCallback = (status: 'idle' | 'waiting' | 'listening') => {
      console.log('üîä Status changed:', status)
      setStatus(status)
    }
    
    const resultCallback = (result: { transcript: string; isFinal: boolean; alternatives?: string[] }) => {
      console.log('üéØ Result received:', { 
        transcript: result.transcript, 
        isFinal: result.isFinal,
        hasEditor: !!editorRef.current 
      })
      // üÜï FASE 6: Usar refs est√°veis
      if (result.isFinal) {
        handleFinalTranscriptRef.current(result.transcript)
      } else {
        handleInterimTranscriptRef.current(result.transcript)
      }
    }

    speechService.setOnStatus(statusCallback)
    speechService.setOnResult(resultCallback)
    
    console.log('‚úì Voice callbacks configured for useDictation')

    return () => {
      speechService.removeOnStatus(statusCallback)
      speechService.removeOnResult(resultCallback)
      speechService.stopListening()
    }
  }, []) // üÜï FASE 6: Array de depend√™ncias vazio - callbacks estabilizados

  /**
   * üÜï FASE 8: Privacidade - parar microfone quando aba n√£o est√° vis√≠vel
   */
  useEffect(() => {
    const handleVisibilityChange = () => {
      if (document.hidden && isActive) {
        console.log('üîí Tab hidden - stopping dictation for privacy')
        stopDictation()
        // üÜï FASE 8: Limpar √°udio da mem√≥ria
        audioChunksRef.current = []
        toast.info('Ditado pausado (aba em segundo plano)')
      }
    }

    document.addEventListener('visibilitychange', handleVisibilityChange)

    return () => {
      document.removeEventListener('visibilitychange', handleVisibilityChange)
    }
  }, [isActive, stopDictation])

  /**
   * Toggle Whisper
   */
  const toggleWhisper = useCallback(() => {
    // Check if user has enough credits before enabling
    if (!isWhisperEnabled && !hasEnoughCredits) {
      toast.error('‚ú® Ative o Whisper AI para transcri√ß√£o m√©dica precisa. Termos como "hepatomegalia" e "BI-RADS" s√£o refinados automaticamente.', {
        duration: 6000,
      })
      return
    }
    
    setIsWhisperEnabled(prev => {
      const newState = !prev
      toast.info(newState ? 'Whisper AI ativado ‚úÖ' : 'Whisper AI desativado ‚è∏Ô∏è')
      return newState
    })
  }, [isWhisperEnabled, hasEnoughCredits])

  /**
   * Cleanup ao desmontar
   */
  useEffect(() => {
    return () => {
      stopAudioRecording()
    }
  }, [stopAudioRecording])

  return {
    isActive,
    status,
    startDictation,
    stopDictation,
    
    // Whisper features integradas
    isWhisperEnabled,
    toggleWhisper,
    isTranscribing,
    whisperStats,
  }
}
